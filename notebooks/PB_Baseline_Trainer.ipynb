{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7951bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcorr\\anaconda3\\envs\\asset_pricing\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.data_utils.loader import DataLoader\n",
    "from src.models.config import ModelConfig\n",
    "from src.models.trainer import AssetPricingTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1691a137",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    # Linear Methods\n",
    "    ModelConfig.OLS_Huber(name=\"OLS+H\", feature_set=\"all\", trials=50),\n",
    "    ModelConfig.OLS_Huber(name=\"OLS-3+H\", feature_set=\"ff3\", trials=50),\n",
    "    ModelConfig.ElasticNet_Huber(name=\"ENet+H\", feature_set=\"all\", trials=50),\n",
    "    # ModelConfig.PCR(name=\"PCR\", feature_set=\"ff3\", trials=30),\n",
    "\n",
    "    # # Tree based methods\n",
    "    # ModelConfig.RandomForest(name=\"RF\", feature_set=\"ff3\", trials=30),\n",
    "    # ModelConfig.XGBoost(name=\"XGB+H\", feature_set=\"ff3\", trials=30),\n",
    "\n",
    "    # # Neural Networks\n",
    "    # ModelConfig.MLP(name=\"NN1\", feature_set=\"all\", n_hidden_layers=1, trials=50),\n",
    "    # ModelConfig.MLP(name=\"NN2\", feature_set=\"all\", n_hidden_layers=2, trials=50),\n",
    "    # ModelConfig.MLP(name=\"NN3\", feature_set=\"all\", n_hidden_layers=3, trials=50),\n",
    "    # ModelConfig.MLP(name=\"NN4\", feature_set=\"all\", n_hidden_layers=4, trials=50),\n",
    "    # ModelConfig.MLP(name=\"NN5\", feature_set=\"all\", n_hidden_layers=5, trials=50),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d95a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-10 16:03:07] [INFO] [ParquetLoader] REQ: Loading ALL columns...\n",
      "[2026-02-10 16:03:07] [INFO] [ParquetLoader] SOURCE: data/processed/gold_panel\n",
      "[2026-02-10 16:03:22] [INFO] [ParquetLoader] LOAD COMPLETE. Shape: (3280700, 100) | RAM: 1.33 GB\n"
     ]
    }
   ],
   "source": [
    "gold_panel_path = \"data/processed/gold_panel\"\n",
    "\n",
    "loader = DataLoader(data_path = gold_panel_path)\n",
    "df = loader.load_panel_data()\n",
    "\n",
    "trainer = AssetPricingTrainer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e768d8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-10 00:20:12] [INFO] [Trainer] --- STARTING EXPERIMENT: NN1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 00:20:13,674]\u001b[0m Using an existing study with name 'NN1_optimization' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-10 00:20:13] [INFO] [Trainer] [+] SPLITTING DATA\n",
      "[2026-02-10 00:20:13] [INFO] [Trainer] Time Split: 2008-12-31 | Train: 2698578 | Test: 582122\n",
      "[2026-02-10 00:20:14] [INFO] [Trainer] [+] FINDING OPTIMAL HYPERPARAMETERS\n",
      "[2026-02-10 00:20:14] [INFO] [Trainer]    > Checking Optuna state for NN1...\n",
      "[2026-02-10 00:20:14] [INFO] [Trainer]    > Resuming: Found 35 trials. Running 15 more...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 00:29:08,390]\u001b[0m Trial 35 finished with value: 0.017509877681732178 and parameters: {'alpha': 0.02273800255032061, 'learning_rate_init': 0.002228843400719724}. Best is trial 35 with value: 0.017509877681732178.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 00:33:22,380]\u001b[0m Trial 36 finished with value: -0.04161357879638672 and parameters: {'alpha': 8.101399924475002e-05, 'learning_rate_init': 0.0021609700144148204}. Best is trial 35 with value: 0.017509877681732178.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 00:43:43,917]\u001b[0m Trial 37 finished with value: 0.015390455722808838 and parameters: {'alpha': 0.006130948713135182, 'learning_rate_init': 0.0015271574545911335}. Best is trial 35 with value: 0.017509877681732178.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 00:56:48,363]\u001b[0m Trial 38 finished with value: -0.006834745407104492 and parameters: {'alpha': 0.008058134566196883, 'learning_rate_init': 0.0015058794748457197}. Best is trial 35 with value: 0.017509877681732178.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 01:19:10,366]\u001b[0m Trial 39 finished with value: -0.013636350631713867 and parameters: {'alpha': 0.015093375758977939, 'learning_rate_init': 0.0006658419332615762}. Best is trial 35 with value: 0.017509877681732178.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 01:34:48,063]\u001b[0m Trial 40 finished with value: -0.0006110668182373047 and parameters: {'alpha': 0.005353245874269258, 'learning_rate_init': 0.0010792857280203963}. Best is trial 35 with value: 0.017509877681732178.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 01:44:22,324]\u001b[0m Trial 41 finished with value: -0.007979989051818848 and parameters: {'alpha': 0.0011564615475353958, 'learning_rate_init': 0.0020432633035093887}. Best is trial 35 with value: 0.017509877681732178.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 01:50:53,958]\u001b[0m Trial 42 finished with value: -0.0033636093139648438 and parameters: {'alpha': 0.003272029426526184, 'learning_rate_init': 0.0028148604207623145}. Best is trial 35 with value: 0.017509877681732178.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 02:03:43,727]\u001b[0m Trial 43 finished with value: -0.014861226081848145 and parameters: {'alpha': 0.029573292002259492, 'learning_rate_init': 0.0016313611279409304}. Best is trial 35 with value: 0.017509877681732178.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 02:10:23,329]\u001b[0m Trial 44 finished with value: -0.031623244285583496 and parameters: {'alpha': 0.005979575264142062, 'learning_rate_init': 0.0033753046077124337}. Best is trial 35 with value: 0.017509877681732178.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 02:17:01,437]\u001b[0m Trial 45 finished with value: -0.036756277084350586 and parameters: {'alpha': 0.013473522675498307, 'learning_rate_init': 0.00225680349698433}. Best is trial 35 with value: 0.017509877681732178.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 02:29:11,543]\u001b[0m Trial 46 finished with value: -0.02470850944519043 and parameters: {'alpha': 0.06598719133929745, 'learning_rate_init': 0.00044532859748687447}. Best is trial 35 with value: 0.017509877681732178.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 02:50:31,970]\u001b[0m Trial 47 finished with value: -0.02842879295349121 and parameters: {'alpha': 0.0008312207397077691, 'learning_rate_init': 0.0017506607937064643}. Best is trial 35 with value: 0.017509877681732178.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 02:56:35,961]\u001b[0m Trial 48 finished with value: -0.0004597902297973633 and parameters: {'alpha': 0.04731589677032601, 'learning_rate_init': 0.0027387307534194067}. Best is trial 35 with value: 0.017509877681732178.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 03:14:54,272]\u001b[0m Trial 49 finished with value: -0.052101850509643555 and parameters: {'alpha': 0.000418926305621211, 'learning_rate_init': 0.001342869288219935}. Best is trial 35 with value: 0.017509877681732178.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-10 03:14:54] [INFO] [Trainer]    > Tuning Complete.\n",
      "[2026-02-10 03:14:54] [INFO] [Trainer] [+] LOADING MODEL\n",
      "[2026-02-10 03:14:54] [INFO] [Trainer]    > Training Final Model (NN1)...\n",
      "[2026-02-10 03:35:48] [INFO] [Trainer]    > Saved trained model to src/models/trained\\NN1_20260210_033548.joblib\n",
      "[2026-02-10 03:36:07] [INFO] [Trainer] RESULT NN1: R2_OOS=-0.00494 | RMSE=0.19248\n",
      "[2026-02-10 03:36:07] [INFO] [Trainer] --- STARTING EXPERIMENT: NN2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 03:36:07,995]\u001b[0m A new study created in RDB with name: NN2_optimization\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-10 03:36:07] [INFO] [Trainer] [+] SPLITTING DATA\n",
      "[2026-02-10 03:36:08] [INFO] [Trainer] Time Split: 2008-12-31 | Train: 2698578 | Test: 582122\n",
      "[2026-02-10 03:36:10] [INFO] [Trainer] [+] FINDING OPTIMAL HYPERPARAMETERS\n",
      "[2026-02-10 03:36:10] [INFO] [Trainer]    > Checking Optuna state for NN2...\n",
      "[2026-02-10 03:36:10] [INFO] [Trainer]    > Resuming: Found 0 trials. Running 50 more...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-10 03:40:10,114]\u001b[0m Trial 0 finished with value: -0.7059128284454346 and parameters: {'alpha': 0.0005433670374209586, 'learning_rate_init': 0.0006397920986512874}. Best is trial 0 with value: -0.7059128284454346.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 04:00:30,965]\u001b[0m Trial 1 finished with value: -0.21499931812286377 and parameters: {'alpha': 0.006463241144390132, 'learning_rate_init': 0.0008886623013625115}. Best is trial 1 with value: -0.21499931812286377.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 04:04:04,195]\u001b[0m Trial 2 finished with value: -1.4146487712860107 and parameters: {'alpha': 8.48180002382634e-05, 'learning_rate_init': 0.0005212405506784783}. Best is trial 1 with value: -0.21499931812286377.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 04:18:07,689]\u001b[0m Trial 3 finished with value: -2.648115396499634 and parameters: {'alpha': 0.0007937249050201012, 'learning_rate_init': 0.0002419437390732874}. Best is trial 1 with value: -0.21499931812286377.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 04:40:35,580]\u001b[0m Trial 4 finished with value: -0.7817258834838867 and parameters: {'alpha': 0.00017591499100262135, 'learning_rate_init': 0.0022638489809834856}. Best is trial 1 with value: -0.21499931812286377.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 04:54:37,881]\u001b[0m Trial 5 finished with value: -0.2640237808227539 and parameters: {'alpha': 0.003727188098630709, 'learning_rate_init': 0.000719385710070526}. Best is trial 1 with value: -0.21499931812286377.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 05:00:06,824]\u001b[0m Trial 6 finished with value: -0.523888349533081 and parameters: {'alpha': 0.00018288255151037883, 'learning_rate_init': 0.00036375323329908244}. Best is trial 1 with value: -0.21499931812286377.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 05:04:23,427]\u001b[0m Trial 7 finished with value: -0.6499210596084595 and parameters: {'alpha': 0.00021938727582288743, 'learning_rate_init': 0.00010149932612567278}. Best is trial 1 with value: -0.21499931812286377.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 05:21:07,462]\u001b[0m Trial 8 finished with value: -0.05832266807556152 and parameters: {'alpha': 0.02816096930062667, 'learning_rate_init': 0.002044346264556186}. Best is trial 8 with value: -0.05832266807556152.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 05:36:00,699]\u001b[0m Trial 9 finished with value: -0.39794158935546875 and parameters: {'alpha': 9.949888619790696e-05, 'learning_rate_init': 0.0028017502932374838}. Best is trial 8 with value: -0.05832266807556152.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 05:42:43,423]\u001b[0m Trial 10 finished with value: 0.005152702331542969 and parameters: {'alpha': 0.07085362420095272, 'learning_rate_init': 0.005482831286334266}. Best is trial 10 with value: 0.005152702331542969.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 05:49:23,090]\u001b[0m Trial 11 finished with value: 0.00227510929107666 and parameters: {'alpha': 0.0788729178166752, 'learning_rate_init': 0.007265960909539137}. Best is trial 10 with value: 0.005152702331542969.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 05:55:14,795]\u001b[0m Trial 12 finished with value: 0.004123568534851074 and parameters: {'alpha': 0.08946596426447691, 'learning_rate_init': 0.008482917374311793}. Best is trial 10 with value: 0.005152702331542969.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 06:01:01,175]\u001b[0m Trial 13 finished with value: 0.006484091281890869 and parameters: {'alpha': 0.06911889506880292, 'learning_rate_init': 0.008049951941326846}. Best is trial 13 with value: 0.006484091281890869.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 06:14:42,208]\u001b[0m Trial 14 finished with value: -0.18738198280334473 and parameters: {'alpha': 1.5285992546587617e-05, 'learning_rate_init': 0.00445030440039466}. Best is trial 13 with value: 0.006484091281890869.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 06:25:41,419]\u001b[0m Trial 15 finished with value: -0.012072205543518066 and parameters: {'alpha': 0.010868866070867074, 'learning_rate_init': 0.004797401682731174}. Best is trial 13 with value: 0.006484091281890869.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 06:31:43,119]\u001b[0m Trial 16 finished with value: -0.0003706216812133789 and parameters: {'alpha': 0.026625927598711456, 'learning_rate_init': 0.00884350946694499}. Best is trial 13 with value: 0.006484091281890869.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 06:40:56,652]\u001b[0m Trial 17 finished with value: -0.06495261192321777 and parameters: {'alpha': 0.0028453300855313546, 'learning_rate_init': 0.0013980271353246036}. Best is trial 13 with value: 0.006484091281890869.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 06:48:52,263]\u001b[0m Trial 18 finished with value: -0.005598783493041992 and parameters: {'alpha': 0.021538309428734904, 'learning_rate_init': 0.004726153485053936}. Best is trial 13 with value: 0.006484091281890869.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 07:03:12,265]\u001b[0m Trial 19 finished with value: -0.07813477516174316 and parameters: {'alpha': 0.002086418428028975, 'learning_rate_init': 0.002929358986063294}. Best is trial 13 with value: 0.006484091281890869.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 07:09:02,544]\u001b[0m Trial 20 finished with value: 0.006317555904388428 and parameters: {'alpha': 0.0425925929523536, 'learning_rate_init': 0.005801747341178037}. Best is trial 13 with value: 0.006484091281890869.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 07:15:05,821]\u001b[0m Trial 21 finished with value: 0.0011650919914245605 and parameters: {'alpha': 0.0561714829950383, 'learning_rate_init': 0.005715111729178172}. Best is trial 13 with value: 0.006484091281890869.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 07:22:00,268]\u001b[0m Trial 22 finished with value: 0.009406745433807373 and parameters: {'alpha': 0.014089713670912612, 'learning_rate_init': 0.009682525433113785}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 07:28:35,636]\u001b[0m Trial 23 finished with value: 0.006169378757476807 and parameters: {'alpha': 0.013438057218913406, 'learning_rate_init': 0.009395143522650739}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 07:46:42,118]\u001b[0m Trial 24 finished with value: -0.03984189033508301 and parameters: {'alpha': 0.03367148164076107, 'learning_rate_init': 0.0014160673979393653}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 07:58:36,943]\u001b[0m Trial 25 finished with value: -0.03327035903930664 and parameters: {'alpha': 0.01233133596159104, 'learning_rate_init': 0.003958009920792606}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 08:06:57,840]\u001b[0m Trial 26 finished with value: -0.06388282775878906 and parameters: {'alpha': 0.005388378790252238, 'learning_rate_init': 0.006781664508293077}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 08:12:35,779]\u001b[0m Trial 27 finished with value: 0.0027660727500915527 and parameters: {'alpha': 0.04464473087079925, 'learning_rate_init': 0.009996883536856571}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 08:21:56,933]\u001b[0m Trial 28 finished with value: 0.003128230571746826 and parameters: {'alpha': 0.018114998089530803, 'learning_rate_init': 0.003482394416562658}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 08:51:04,042]\u001b[0m Trial 29 finished with value: -0.267716646194458 and parameters: {'alpha': 0.0014732374309475583, 'learning_rate_init': 0.0016040950567002398}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 08:58:10,843]\u001b[0m Trial 30 finished with value: -0.024900436401367188 and parameters: {'alpha': 0.00738049336314398, 'learning_rate_init': 0.00613418008566214}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 09:03:53,599]\u001b[0m Trial 31 finished with value: -0.013274431228637695 and parameters: {'alpha': 0.01196321870345154, 'learning_rate_init': 0.00964137149854256}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 09:11:23,694]\u001b[0m Trial 32 finished with value: 0.005733132362365723 and parameters: {'alpha': 0.05115933952137996, 'learning_rate_init': 0.00700676395061257}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 09:18:14,760]\u001b[0m Trial 33 finished with value: 0.0021060705184936523 and parameters: {'alpha': 0.09607400801858657, 'learning_rate_init': 0.006984528321245238}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 09:30:02,594]\u001b[0m Trial 34 finished with value: -0.06663620471954346 and parameters: {'alpha': 0.01824704875527624, 'learning_rate_init': 0.0036843479511449446}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 09:49:23,801]\u001b[0m Trial 35 finished with value: -0.22848117351531982 and parameters: {'alpha': 0.006451429420743841, 'learning_rate_init': 0.0010067258334480243}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 09:56:55,463]\u001b[0m Trial 36 finished with value: -0.055152058601379395 and parameters: {'alpha': 0.0007880973264528245, 'learning_rate_init': 0.009994156414763478}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n",
      "\u001b[32m[I 2026-02-10 10:31:26,715]\u001b[0m Trial 37 finished with value: -0.09559023380279541 and parameters: {'alpha': 0.03989996148401019, 'learning_rate_init': 0.00043380071186421284}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n",
      "c:\\Users\\rcorr\\anaconda3\\envs\\asset_pricing\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:792: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "\u001b[32m[I 2026-02-10 11:17:32,149]\u001b[0m Trial 38 finished with value: -0.5742373466491699 and parameters: {'alpha': 0.003879960708349667, 'learning_rate_init': 0.00010591134815132696}. Best is trial 22 with value: 0.009406745433807373.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for exp in experiments:\n",
    "    metrics = trainer.run_experiment(exp)\n",
    "    metrics['model'] = exp.name\n",
    "    results.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc7eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(results)\n",
    "print(\"BASELINE LEADERBOARD\")\n",
    "print(res_df[['model', 'r2_oos', 'rmse']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asset_pricing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
